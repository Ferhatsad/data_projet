{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a618fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('mon_data.csv')\n",
    "df.dropna(subset=['text_image'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2839c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af81e457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_image</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FACTURE\\n\\nLOGO\\n\\nJoanna Binet\\n48 Coubertin\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joanna Binet\\n48 Coubertin\\n31400 Paris\\n\\nFAC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FACTURE\\n\\nMon entreprise : Nom de la société\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joanna Binet\\n48 Coubertin\\n31400 Paris\\n\\nFAC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Payer en ligne &gt;\\nFACTURE No\\n\\nSFIDELI\\n\\nF/0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>Minois Retail Merchants Association\\n\\n36 Sout...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>FEMALE\\n\\n3\\n2 :\\n1\\n\\nMALE\\n\\n3-\\n2\\n\\n3\\nS\\n...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>(B&amp;W) PROTECTED BY MINNESOTA TOBACCO LITIGATIO...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>FF\\n\\nPrincipal Investigator/Program Director ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>thag\\n\\n1 4\\n\\nHARTFORD, CONN.\\nCOURANT\\nD. 17...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1566 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_image  type\n",
       "0     FACTURE\\n\\nLOGO\\n\\nJoanna Binet\\n48 Coubertin\\...     0\n",
       "1     Joanna Binet\\n48 Coubertin\\n31400 Paris\\n\\nFAC...     0\n",
       "2     FACTURE\\n\\nMon entreprise : Nom de la société\\...     0\n",
       "3     Joanna Binet\\n48 Coubertin\\n31400 Paris\\n\\nFAC...     0\n",
       "4     Payer en ligne >\\nFACTURE No\\n\\nSFIDELI\\n\\nF/0...     0\n",
       "...                                                 ...   ...\n",
       "1603  Minois Retail Merchants Association\\n\\n36 Sout...    16\n",
       "1604  FEMALE\\n\\n3\\n2 :\\n1\\n\\nMALE\\n\\n3-\\n2\\n\\n3\\nS\\n...    20\n",
       "1605  (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATIO...    14\n",
       "1606  FF\\n\\nPrincipal Investigator/Program Director ...    11\n",
       "1607  thag\\n\\n1 4\\n\\nHARTFORD, CONN.\\nCOURANT\\nD. 17...    17\n",
       "\n",
       "[1566 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.iloc[:,[-1,3]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7edc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc2875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84125a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update([\".\" , \",\",':'])\n",
    "stop_words=list(stop_words)\n",
    "stop_words.extend(stopwords.words('french'))\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(str(w).lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
    "\n",
    "    # remove stopword\n",
    "    mots = word_tokenize(w.strip())\n",
    "    mots = [mot for mot in mots if mot not in stop_words]\n",
    "    return ' '.join(mots).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aec133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text_image = df.text_image.apply(lambda x :preprocess_sentence(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f9654c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89478d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=200,\n",
    "                     alpha=0.03, \n",
    "                     negative=10,\n",
    "                     seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.regexp import  RegexpTokenizer\n",
    "stop_words = set(stopwords.words('french'))\n",
    "stop_words.update([\".\" , \",\",':'])\n",
    "tokenizer=RegexpTokenizer((\"[a-zA-Zé]{3,}\"))\n",
    "def stop_words_filtering(liste):\n",
    "    liste_new=[]\n",
    "    for i in liste:\n",
    "        if i not in stop_words:\n",
    "            liste_new.append(i)\n",
    "        else:\n",
    "            continue \n",
    "    return liste_new\n",
    "sentences=[]\n",
    "for txt in df.text_image:\n",
    "    mots=stop_words_filtering(tokenizer.tokenize(str(txt).lower()))\n",
    "    sentences.append(mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3d78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1106\n",
      "['date', 'total', 'tobacco', 'new', 'research', 'net', 'source', 'nom', 'montant', 'non', 'university', 'salaire', 'cotisations', 'base', 'one', 'may', 'smoking', 'taux', 'valeur', 'cigarette', 'revenu', 'sociale', 'year', 'code', 'paie', 'name', 'health', 'facture', 'www', 'securite', 'cigarettes', 'heures', 'number', 'bulletin', 'csg', 'employeur', 'service', 'impot', 'paris', 'york', 'information', 'smoke', 'state', 'brut', 'company', 'payer', 'carte', 'imposable', 'use', 'american', 'page', 'per', 'study', 'rue', 'two', 'inc', 'time', 'program', 'brand', 'industrydocuments', 'https', 'chomage', 'adresse', 'paiement', 'also', 'tva', 'report', 'complementaire', 'data', 'contributions', 'years', 'national', 'part', 'assurance', 'medical', 'medicine', 'product', 'public', 'sante', 'would', 'francaise', 'telephone', 'retraite', 'general', 'smokers', 'department', 'please', 'high', 'amount', 'cancer', 'subject', 'form', 'office', 'description', 'address', 'client', 'age', 'morris', 'institute', 'low', 'used', 'maladie', 'travail', 'republique', 'order', 'type', 'school', 'first', 'present', 'avenue', 'air', 'group', 'ucst', 'periode', 'products', 'washington', 'include', 'conges', 'philip', 'deductible', 'period', 'check', 'development', 'paper', 'tranche', 'docs', 'food', 'signature', 'autres', 'pack', 'section', 'current', 'many', 'numero', 'college', 'project', 'cell', 'given', 'box', 'deces', 'filter', 'france', 'services', 'city', 'see', 'com', 'made', 'euros', 'last', 'mois', 'president', 'following', 'nicotine', 'janvier', 'martin', 'committee', 'crds', 'march', 'prix', 'lung', 'day', 'remuneration', 'street', 'meeting', 'work', 'director', 'weight', 'salarie', 'area', 'results', 'business', 'center', 'birth', 'less', 'compte', 'edu', 'cumul', 'contrat', 'said', 'people', 'der', 'association', 'level', 'plus', 'cot', 'board', 'jours', 'education', 'place', 'nombre', 'like', 'material', 'siret', 'nutrition', 'test', 'available', 'conditions', 'cost', 'patronales', 'studies', 'paye', 'local', 'avant', 'convention', 'states', 'epa', 'well', 'water', 'week', 'received', 'passport', 'application', 'due', 'rjr', 'emploi', 'del', 'value', 'dues', 'change', 'list', 'pages', 'united', 'identite', 'publications', 'industry', 'card', 'call', 'share', 'expenses', 'division', 'biology', 'sales', 'three', 'nature', 'rate', 'ttc', 'deplafonnee', 'professional', 'payes', 'among', 'journal', 'chemical', 'tar', 'invalidite', 'nationale', 'sexe', 'document', 'dont', 'risk', 'title', 'tax', 'professor', 'june', 'entreprise', 'residence', 'collective', 'usage', 'mail', 'postale', 'fax', 'disease', 'taste', 'april', 'lieu', 'social', 'charges', 'passeport', 'pris', 'could', 'usa', 'plafonnee', 'must', 'make', 'travel', 'urssaf', 'associate', 'days', 'cas', 'table', 'nationalite', 'personnel', 'sans', 'winston', 'payment', 'account', 'market', 'famille', 'contact', 'society', 'long', 'government', 'children', 'using', 'distribution', 'case', 'including', 'contribution', 'key', 'status', 'specific', 'exposure', 'note', 'eduldocs', 'september', 'activity', 'phone', 'fra', 'women', 'jour', 'titulaire', 'management', 'indemnite', 'position', 'points', 'copy', 'prenom', 'special', 'effects', 'members', 'point', 'assistant', 'prime', 'pre', 'end', 'science', 'plan', 'august', 'ing', 'advertising', 'marie', 'process', 'hospital', 'good', 'way', 'flavor', 'october', 'experience', 'publication', 'yes', 'reference', 'additional', 'attached', 'vice', 'income', 'marketing', 'deplacements', 'average', 'right', 'protein', 'growth', 'groups', 'volume', 'con', 'international', 'free', 'diet', 'pat', 'drug', 'country', 'review', 'previous', 'based', 'complete', 'times', 'july', 'different', 'staff', 'federal', 'questions', 'calcium', 'und', 'best', 'length', 'companies', 'today', 'etre', 'back', 'fiche', 'programs', 'levels', 'training', 'preleve', 'foods', 'designation', 'costs', 'pro', 'required', 'however', 'associated', 'possible', 'within', 'sociales', 'action', 'ville', 'kool', 'retenues', 'agency', 'ohio', 'dept', 'found', 'sent', 'salariales', 'person', 'production', 'xxx', 'december', 'solde', 'clinical', 'monthly', 'november', 'survey', 'biochemistry', 'site', 'price', 'edf', 'sheet', 'direct', 'cent', 'control', 'virginia', 'percent', 'prepared', 'factors', 'full', 'category', 'sal', 'issue', 'salem', 'prenoms', 'acquis', 'transport', 'duree', 'rtbpfw', 'sugar', 'loss', 'final', 'approved', 'progress', 'record', 'senior', 'since', 'without', 'interest', 'evaluation', 'lights', 'determine', 'chicago', 'get', 'agreement', 'venue', 'budget', 'several', 'increase', 'need', 'sex', 'fire', 'formation', 'result', 'request', 'provide', 'maladies', 'reglement', 'support', 'quality', 'hand', 'human', 'consumer', 'molecular', 'schedule', 'men', 'increased', 'take', 'cells', 'notice', 'law', 'cases', 'added', 'display', 'scientific', 'chemicals', 'various', 'anciennete', 'cadre', 'bank', 'dry', 'important', 'reported', 'standard', 'promotion', 'set', 'die', 'proposed', 'february', 'ape', 'council', 'chemistry', 'member', 'mensuel', 'dupont', 'reduction', 'values', 'park', 'mode', 'effect', 'names', 'informations', 'invoice', 'legal', 'thank', 'every', 'charge', 'sodium', 'analysis', 'domicile', 'sciences', 'past', 'personal', 'begin', 'taxe', 'major', 'annual', 'canada', 'jean', 'return', 'rev', 'much', 'vieillesse', 'moviles', 'california', 'david', 'ind', 'professionnelles', 'four', 'accident', 'rod', 'horaire', 'let', 'held', 'environmental', 'gene', 'quarter', 'principal', 'evidence', 'comments', 'dated', 'purpose', 'attestation', 'samples', 'manager', 'materials', 'active', 'experimental', 'etc', 'louis', 'tion', 'jan', 'heart', 'mif', 'press', 'india', 'dear', 'preparation', 'least', 'records', 'related', 'approval', 'second', 'world', 'media', 'laboratory', 'dec', 'assets', 'foundation', 'menthol', 'bureau', 'help', 'reynolds', 'even', 'gaelle', 'eur', 'electricite', 'january', 'north', 'verse', 'entre', 'ask', 'administration', 'hormone', 'funds', 'san', 'none', 'produced', 'read', 'dut', 'white', 'location', 'rat', 'single', 'limited', 'htps', 'packs', 'soft', 'union', 'releve', 'activities', 'cedex', 'response', 'item', 'million', 'prioritization', 'common', 'annee', 'internet', 'population', 'next', 'compared', 'grade', 'fellow', 'system', 'accidents', 'cause', 'side', 'line', 'necessary', 'another', 'applicable', 'matricule', 'robert', 'unitaire', 'tipping', 'cours', 'role', 'significant', 'naissance', 'students', 'illinois', 'allegement', 'temps', 'sample', 'paid', 'others', 'question', 'post', 'addition', 'discussion', 'month', 'america', 'notes', 'details', 'animals', 'conference', 'art', 'room', 'provided', 'trade', 'fnal', 'correct', 'john', 'indemnites', 'care', 'frais', 'naf', 'maternite', 'prelevement', 'suppression', 'bap', 'technology', 'regular', 'missouri', 'solidarite', 'degree', 'building', 'months', 'rates', 'individuals', 'acid', 'effective', 'tumor', 'problem', 'chairman', 'home', 'death', 'elements', 'county', 'ans', 'physical', 'give', 'old', 'deplacement', 'substances', 'capital', 'upon', 'life', 'potential', 'societe', 'virement', 'basic', 'amounts', 'statement', 'limitation', 'items', 'fda', 'still', 'iban', 'although', 'faire', 'male', 'shown', 'coupon', 'currently', 'true', 'annuel', 'marlboro', 'taille', 'weeks', 'contract', 'cotisation', 'charles', 'higher', 'actual', 'continue', 'salaries', 'female', 'show', 'quantite', 'delivery', 'nov', 'model', 'terminate', 'field', 'unit', 'term', 'salarial', 'filler', 'merci', 'difference', 'nnk', 'incapacite', 'william', 'investigator', 'panel', 'graduate', 'great', 'consommation', 'terms', 'mouse', 'retenue', 'traitement', 'merit', 'brute', 'range', 'inter', 'financial', 'trimegestone', 'carton', 'tout', 'might', 'appel', 'submitted', 'mme', 'mouth', 'road', 'taken', 'doit', 'answer', 'thomas', 'house', 'variety', 'segment', 'family', 'eau', 'depuis', 'whether', 'supplementaires', 'anti', 'printing', 'profit', 'know', 'basis', 'approach', 'article', 'hotel', 'boston', 'face', 'itc', 'message', 'placed', 'bancaire', 'working', 'areas', 'size', 'procedure', 'class', 'confidential', 'mortality', 'brown', 'diseases', 'det', 'persons', 'ags', 'equity', 'extremely', 'gain', 'relationship', 'fur', 'ppm', 'pressure', 'expected', 'authorized', 'offer', 'projects', 'extreme', 'store', 'taxes', 'maelys', 'dose', 'fund', 'advisory', 'shares', 'independent', 'sous', 'figure', 'revised', 'green', 'min', 'massachusetts', 'parts', 'hazard', 'smith', 'content', 'estradiol', 'banque', 'act', 'either', 'cout', 'los', 'fish', 'completed', 'young', 'postdoctoral', 'letter', 'saint', 'station', 'king', 'small', 'agent', 'send', 'evolution', 'original', 'course', 'documents', 'ten', 'little', 'references', 'problems', 'recent', 'reason', 'court', 'black', 'associates', 'private', 'global', 'future', 'requested', 'mars', 'containing', 'prevoyance', 'main', 'cfr', 'countries', 'south', 'bic', 'absence', 'regulation', 'pay', 'chf', 'stock', 'reports', 'buy', 'subjects', 'opinion', 'den', 'nil', 'meetings', 'ofl', 'paul', 'operations', 'come', 'individual', 'retard', 'expense', 'proposal', 'milk', 'daily', 'email', 'selected', 'safety', 'brain', 'making', 'planning', 'doctors', 'prior', 'representative', 'industrial', 'corporate', 'corporation', 'schools', 'involved', 'carbon', 'say', 'better', 'interet', 'assistance', 'cumuls', 'iii', 'toxicology', 'estimated', 'greater', 'james', 'zip', 'consumers', 'newport', 'money', 'brands', 'responsible', 'impact', 'indicate', 'news', 'displays', 'later', 'central', 'estimate', 'postal', 'mice', 'differences', 'ads', 'west', 'identification', 'aut', 'cheque', 'third', 'biological', 'lorillard', 'cfc', 'tissue', 'cette', 'determined', 'commerce', 'job', 'ultra', 'glue', 'community', 'five', 'acc', 'impots', 'want', 'issues', 'avantages', 'police', 'target', 'receive', 'early', 'summary', 'richmond', 'allow', 'executive', 'univ', 'droits', 'included', 'retail', 'run', 'fact', 'doc', 'fin', 'dates', 'pierre', 'wednesday', 'netapayer', 'man', 'familiales', 'echelon', 'focus', 'indice', 'organization', 'rubrique', 'dollars', 'instructions', 'florida', 'likely', 'prevention', 'liee', 'print', 'adult', 'payees', 'dna', 'plug', 'apec', 'going', 'nitrogen', 'large', 'exposed', 'says', 'primes', 'lower', 'plafond', 'dust', 'tablets', 'vitro', 'nih', 'land', 'congress', 'supp', 'trial', 'grams', 'cont', 'percentage', 'introduction', 'sup', 'warning', 'indicated', 'camel', 'lab', 'hors', 'toulouse', 'recently', 'friday', 'conducted', 'ment', 'method', 'cash', 'entrez', 'courrier', 'wall', 'express', 'payable', 'cover', 'written', 'enfants', 'texas', 'shall', 'expenditures', 'academy', 'technical', 'libelle', 'official', 'niveau', 'entree', 'adhesive', 'continued', 'patients', 'logo', 'normal', 'numbers', 'presence', 'urgence', 'yield', 'totale', 'salariale', 'deduire', 'leading', 'classification', 'specification', 'honors', 'prochaine', 'vers', 'cpn', 'factor', 'personnalise', 'slight', 'respect', 'stores', 'mark', 'systems', 'chain', 'enclosed', 'chronological', 'reduced', 'vol', 'personnes', 'activite', 'weekly', 'designed', 'parents', 'real', 'certain', 'suite', 'established', 'plant', 'activation', 'fig', 'grant', 'michigan', 'ref', 'done', 'look', 'departement', 'pays', 'papers', 'scientist', 'mar', 'comment', 'image', 'open', 'mean', 'objective', 'salt']\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=100000)\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a85e5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "v=[]\n",
    "df_n=pd.DataFrame(columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ed621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eaa91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for e,j in enumerate(df.text_image):\n",
    "    mots=stop_words_filtering(tokenizer.tokenize(str(j).lower()))\n",
    "    for i in mots:\n",
    "        if i not in words:\n",
    "             \n",
    "            continue  \n",
    "       \n",
    "        else:\n",
    "            v.append(i)\n",
    "    df_n.loc[e]=' '.join(v)\n",
    "            \n",
    "    v=[]\n",
    "    \n",
    "            \n",
    "#mot_vu=w2v_model.wv['facture']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb4ce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1566,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_n.text[3]\n",
    "df.text_image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36109c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadoun\\AppData\\Local\\Temp\\ipykernel_11980\\1465323158.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  total_donnes.append(text_total/len(txt.split()))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_donnes=[]\n",
    "text_total=np.zeros([200])\n",
    "for txt in df_n.text:\n",
    "    for mots in txt.split():\n",
    "        text_total=text_total+w2v_model.wv[mots]\n",
    "    total_donnes.append(text_total/len(txt.split()))\n",
    "    text_total=np.zeros([200])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f25f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_donnes=pd.DataFrame(np.array(total_donnes))\n",
    "total_donnes['type']=df.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03d31453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001757</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>-0.000235</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>-0.000087</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>-0.000892</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>-0.000631</td>\n",
       "      <td>-0.000091</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-0.000323</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.000602</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>-0.000682</td>\n",
       "      <td>-0.000806</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>-0.000697</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>-0.001783</td>\n",
       "      <td>-0.000384</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001896</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-0.000323</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000183</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.000602</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>-0.000682</td>\n",
       "      <td>-0.000806</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000119</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000208</td>\n",
       "      <td>-0.000709</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>-0.000952</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>-0.001158</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.000628</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.001757  0.000317 -0.000249  0.000227 -0.000235  0.000390 -0.000087   \n",
       "1 -0.001896  0.000027 -0.000242  0.000276 -0.000323  0.000511 -0.000262   \n",
       "2  0.000157  0.001080 -0.000697 -0.000058 -0.000645 -0.000046  0.001049   \n",
       "3 -0.001896  0.000027 -0.000242  0.000276 -0.000323  0.000511 -0.000262   \n",
       "4 -0.000119  0.000430 -0.000700  0.000816  0.000739  0.001116  0.000147   \n",
       "\n",
       "          7         8         9  ...       191       192       193       194  \\\n",
       "0  0.001327 -0.000363  0.000073  ... -0.000311  0.000572 -0.000306  0.001215   \n",
       "1  0.001489 -0.000371  0.000160  ... -0.000183  0.000429 -0.000602  0.001549   \n",
       "2  0.001415  0.000278  0.000421  ... -0.000540 -0.000268 -0.001783 -0.000384   \n",
       "3  0.001489 -0.000371  0.000160  ... -0.000183  0.000429 -0.000602  0.001549   \n",
       "4  0.001740 -0.000074 -0.000277  ... -0.000208 -0.000709  0.000095  0.000993   \n",
       "\n",
       "        195       196       197       198       199  type  \n",
       "0 -0.000892  0.000856 -0.000861 -0.000631 -0.000091   0.0  \n",
       "1 -0.000563  0.000728 -0.000682 -0.000806 -0.000144   0.0  \n",
       "2  0.000113 -0.000458 -0.000062  0.000935 -0.000764   0.0  \n",
       "3 -0.000563  0.000728 -0.000682 -0.000806 -0.000144   0.0  \n",
       "4 -0.000952  0.000984 -0.001158  0.000040 -0.000628   0.0  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_donnes.shape\n",
    "total_donnes.dropna(inplace=True)\n",
    "total_donnes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47842118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98faaa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=total_donnes.drop('type',axis=1)\n",
    "\n",
    "target=total_donnes.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daedb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test=train_test_split(data,target,test_size=0.2,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb8c9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn import svm,model_selection,preprocessing\n",
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c90143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               51456     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 23)                5911      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 57,367\n",
      "Trainable params: 57,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Dropout,Activation\n",
    "model=Sequential()\n",
    "model.add(Dense(units = 256,input_shape=(200,)))\n",
    "model.add((Activation('relu')))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 23, activation = \"softmax\"))\n",
    "model.summary()\n",
    "#clf=linear_model.LogisticRegression(C=)\n",
    "#clf.fit(X_train,y_train)\n",
    "#ac=AdaBoostClassifier(base_estimator=dtc,n_estimators=400)\n",
    "#ac.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cd629ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 1s 11ms/step - loss: 3.0823 - accuracy: 0.2179 - val_loss: 2.9927 - val_accuracy: 0.2364\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.8823 - accuracy: 0.2267 - val_loss: 2.7546 - val_accuracy: 0.2364\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6967 - accuracy: 0.2267 - val_loss: 2.6586 - val_accuracy: 0.2364\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6559 - accuracy: 0.2267 - val_loss: 2.6605 - val_accuracy: 0.2364\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6549 - accuracy: 0.2267 - val_loss: 2.6592 - val_accuracy: 0.2364\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6530 - accuracy: 0.2267 - val_loss: 2.6586 - val_accuracy: 0.2364\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6474 - accuracy: 0.2267 - val_loss: 2.6552 - val_accuracy: 0.2364\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6421 - accuracy: 0.2267 - val_loss: 2.6509 - val_accuracy: 0.2364\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6448 - accuracy: 0.2267 - val_loss: 2.6474 - val_accuracy: 0.2364\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6341 - accuracy: 0.2267 - val_loss: 2.6443 - val_accuracy: 0.2364\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6301 - accuracy: 0.2267 - val_loss: 2.6385 - val_accuracy: 0.2364\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6272 - accuracy: 0.2276 - val_loss: 2.6317 - val_accuracy: 0.2364\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6224 - accuracy: 0.2276 - val_loss: 2.6250 - val_accuracy: 0.2364\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6141 - accuracy: 0.2276 - val_loss: 2.6213 - val_accuracy: 0.2403\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.6059 - accuracy: 0.2296 - val_loss: 2.6156 - val_accuracy: 0.2403\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5990 - accuracy: 0.2296 - val_loss: 2.6052 - val_accuracy: 0.2403\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5880 - accuracy: 0.2364 - val_loss: 2.5975 - val_accuracy: 0.2442\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5846 - accuracy: 0.2364 - val_loss: 2.5909 - val_accuracy: 0.2519\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5721 - accuracy: 0.2364 - val_loss: 2.5801 - val_accuracy: 0.2558\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5619 - accuracy: 0.2481 - val_loss: 2.5675 - val_accuracy: 0.2713\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5560 - accuracy: 0.2481 - val_loss: 2.5562 - val_accuracy: 0.2829\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5414 - accuracy: 0.2704 - val_loss: 2.5459 - val_accuracy: 0.2868\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5361 - accuracy: 0.2802 - val_loss: 2.5354 - val_accuracy: 0.3062\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5205 - accuracy: 0.2918 - val_loss: 2.5228 - val_accuracy: 0.2984\n",
      "Epoch 25/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.5081 - accuracy: 0.2811 - val_loss: 2.5096 - val_accuracy: 0.3140\n",
      "Epoch 26/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.4923 - accuracy: 0.3123 - val_loss: 2.4947 - val_accuracy: 0.3178\n",
      "Epoch 27/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.4834 - accuracy: 0.3113 - val_loss: 2.4816 - val_accuracy: 0.3217\n",
      "Epoch 28/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.4656 - accuracy: 0.3152 - val_loss: 2.4642 - val_accuracy: 0.3411\n",
      "Epoch 29/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.4572 - accuracy: 0.3385 - val_loss: 2.4527 - val_accuracy: 0.3450\n",
      "Epoch 30/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.4401 - accuracy: 0.3288 - val_loss: 2.4401 - val_accuracy: 0.4031\n",
      "Epoch 31/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.4262 - accuracy: 0.3648 - val_loss: 2.4303 - val_accuracy: 0.4031\n",
      "Epoch 32/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.4158 - accuracy: 0.3706 - val_loss: 2.4139 - val_accuracy: 0.4031\n",
      "Epoch 33/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.4005 - accuracy: 0.3813 - val_loss: 2.3982 - val_accuracy: 0.4031\n",
      "Epoch 34/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.3829 - accuracy: 0.3969 - val_loss: 2.3844 - val_accuracy: 0.4186\n",
      "Epoch 35/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.3728 - accuracy: 0.3969 - val_loss: 2.3697 - val_accuracy: 0.4109\n",
      "Epoch 36/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.3484 - accuracy: 0.4027 - val_loss: 2.3574 - val_accuracy: 0.4031\n",
      "Epoch 37/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.3459 - accuracy: 0.3998 - val_loss: 2.3494 - val_accuracy: 0.4264\n",
      "Epoch 38/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.3311 - accuracy: 0.4339 - val_loss: 2.3371 - val_accuracy: 0.4767\n",
      "Epoch 39/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.3238 - accuracy: 0.4105 - val_loss: 2.3221 - val_accuracy: 0.4186\n",
      "Epoch 40/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.3081 - accuracy: 0.4144 - val_loss: 2.3130 - val_accuracy: 0.4496\n",
      "Epoch 41/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2829 - accuracy: 0.4251 - val_loss: 2.2995 - val_accuracy: 0.4806\n",
      "Epoch 42/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2753 - accuracy: 0.4484 - val_loss: 2.2844 - val_accuracy: 0.4690\n",
      "Epoch 43/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2611 - accuracy: 0.4484 - val_loss: 2.2740 - val_accuracy: 0.4845\n",
      "Epoch 44/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2479 - accuracy: 0.4407 - val_loss: 2.2599 - val_accuracy: 0.4729\n",
      "Epoch 45/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2314 - accuracy: 0.4397 - val_loss: 2.2497 - val_accuracy: 0.4806\n",
      "Epoch 46/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2188 - accuracy: 0.4601 - val_loss: 2.2405 - val_accuracy: 0.4806\n",
      "Epoch 47/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.2094 - accuracy: 0.4669 - val_loss: 2.2289 - val_accuracy: 0.4806\n",
      "Epoch 48/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.1977 - accuracy: 0.4660 - val_loss: 2.2166 - val_accuracy: 0.4922\n",
      "Epoch 49/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.1844 - accuracy: 0.4679 - val_loss: 2.2053 - val_accuracy: 0.4961\n",
      "Epoch 50/50\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 2.1712 - accuracy: 0.4708 - val_loss: 2.1970 - val_accuracy: 0.4922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a08ed1b820>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.fit(X_train,y_train,epochs = 50, batch_size = 32,validation_data = [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cb7563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 2.1970 - accuracy: 0.4922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.1969828605651855, 0.4922480583190918]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
