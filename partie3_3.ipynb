{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a618fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('mon_data.csv')\n",
    "df.dropna(subset=['text_image'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e4c2839c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename          0\n",
       "url             796\n",
       "description     796\n",
       "type              0\n",
       "filepath          0\n",
       "scraped           0\n",
       "old_filename    796\n",
       "old_filepath      0\n",
       "text_image        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af81e457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_image</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FACTURE\\n\\nLOGO\\n\\nJoanna Binet\\n48 Coubertin\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joanna Binet\\n48 Coubertin\\n31400 Paris\\n\\nFAC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FACTURE\\n\\nMon entreprise : Nom de la société\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joanna Binet\\n48 Coubertin\\n31400 Paris\\n\\nFAC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Payer en ligne &gt;\\nFACTURE No\\n\\nSFIDELI\\n\\nF/0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>Minois Retail Merchants Association\\n\\n36 Sout...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>FEMALE\\n\\n3\\n2 :\\n1\\n\\nMALE\\n\\n3-\\n2\\n\\n3\\nS\\n...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>(B&amp;W) PROTECTED BY MINNESOTA TOBACCO LITIGATIO...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>FF\\n\\nPrincipal Investigator/Program Director ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>thag\\n\\n1 4\\n\\nHARTFORD, CONN.\\nCOURANT\\nD. 17...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1566 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_image  type\n",
       "0     FACTURE\\n\\nLOGO\\n\\nJoanna Binet\\n48 Coubertin\\...     0\n",
       "1     Joanna Binet\\n48 Coubertin\\n31400 Paris\\n\\nFAC...     0\n",
       "2     FACTURE\\n\\nMon entreprise : Nom de la société\\...     0\n",
       "3     Joanna Binet\\n48 Coubertin\\n31400 Paris\\n\\nFAC...     0\n",
       "4     Payer en ligne >\\nFACTURE No\\n\\nSFIDELI\\n\\nF/0...     0\n",
       "...                                                 ...   ...\n",
       "1603  Minois Retail Merchants Association\\n\\n36 Sout...    16\n",
       "1604  FEMALE\\n\\n3\\n2 :\\n1\\n\\nMALE\\n\\n3-\\n2\\n\\n3\\nS\\n...    20\n",
       "1605  (B&W) PROTECTED BY MINNESOTA TOBACCO LITIGATIO...    14\n",
       "1606  FF\\n\\nPrincipal Investigator/Program Director ...    11\n",
       "1607  thag\\n\\n1 4\\n\\nHARTFORD, CONN.\\nCOURANT\\nD. 17...    17\n",
       "\n",
       "[1566 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.iloc[:,[-1,3]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7edc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc2875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84125a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(str(w).lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
    "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
    "\n",
    "    # remove stopword\n",
    "    mots = word_tokenize(w.strip())\n",
    "    mots = [mot for mot in mots if mot not in stop_words]\n",
    "    return ' '.join(mots).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aec133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text_image = df.text_image.apply(lambda x :preprocess_sentence(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f9654c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "89478d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=200,\n",
    "                     alpha=0.03, \n",
    "                     negative=10,\n",
    "                     seed=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6d1ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.regexp import  RegexpTokenizer\n",
    "stop_words = set(stopwords.words('french'))\n",
    "stop_words.update([\".\" , \",\",':'])\n",
    "tokenizer=RegexpTokenizer((\"[a-zA-Zé]{3,}\"))\n",
    "def stop_words_filtering(liste):\n",
    "    liste_new=[]\n",
    "    for i in liste:\n",
    "        if i not in stop_words:\n",
    "            liste_new.append(i)\n",
    "        else:\n",
    "            continue \n",
    "    return liste_new\n",
    "sentences=[]\n",
    "for txt in df.text_image:\n",
    "    mots=stop_words_filtering(tokenizer.tokenize(str(txt).lower()))\n",
    "    sentences.append(mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2a3d78dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1106\n",
      "['date', 'total', 'tobacco', 'new', 'research', 'net', 'source', 'nom', 'montant', 'non', 'university', 'salaire', 'cotisations', 'base', 'one', 'may', 'smoking', 'taux', 'valeur', 'cigarette', 'revenu', 'sociale', 'year', 'code', 'paie', 'name', 'health', 'facture', 'www', 'securite', 'cigarettes', 'heures', 'number', 'bulletin', 'csg', 'employeur', 'service', 'impot', 'paris', 'york', 'information', 'smoke', 'state', 'brut', 'company', 'payer', 'carte', 'imposable', 'use', 'american', 'page', 'per', 'study', 'rue', 'two', 'inc', 'time', 'program', 'brand', 'industrydocuments', 'https', 'chomage', 'adresse', 'paiement', 'also', 'tva', 'report', 'complementaire', 'data', 'contributions', 'years', 'national', 'part', 'assurance', 'medical', 'medicine', 'product', 'public', 'sante', 'would', 'francaise', 'telephone', 'retraite', 'general', 'smokers', 'department', 'please', 'high', 'amount', 'cancer', 'subject', 'form', 'office', 'description', 'address', 'client', 'age', 'morris', 'institute', 'low', 'used', 'maladie', 'travail', 'republique', 'order', 'type', 'school', 'first', 'present', 'avenue', 'air', 'group', 'ucst', 'periode', 'products', 'washington', 'include', 'conges', 'philip', 'deductible', 'period', 'check', 'development', 'paper', 'tranche', 'docs', 'food', 'signature', 'autres', 'pack', 'section', 'current', 'many', 'numero', 'college', 'project', 'cell', 'given', 'box', 'deces', 'filter', 'france', 'services', 'city', 'see', 'com', 'made', 'euros', 'last', 'mois', 'president', 'following', 'nicotine', 'janvier', 'martin', 'committee', 'crds', 'march', 'prix', 'lung', 'day', 'remuneration', 'street', 'meeting', 'work', 'director', 'weight', 'salarie', 'area', 'results', 'business', 'center', 'birth', 'less', 'compte', 'edu', 'cumul', 'contrat', 'said', 'people', 'der', 'association', 'level', 'plus', 'cot', 'board', 'jours', 'education', 'place', 'nombre', 'like', 'material', 'siret', 'nutrition', 'test', 'available', 'conditions', 'cost', 'patronales', 'studies', 'paye', 'local', 'avant', 'convention', 'states', 'epa', 'well', 'water', 'week', 'received', 'passport', 'application', 'due', 'rjr', 'emploi', 'del', 'value', 'dues', 'change', 'list', 'pages', 'united', 'identite', 'publications', 'industry', 'card', 'call', 'share', 'expenses', 'division', 'biology', 'sales', 'three', 'nature', 'rate', 'ttc', 'deplafonnee', 'professional', 'payes', 'among', 'journal', 'chemical', 'tar', 'invalidite', 'nationale', 'sexe', 'document', 'dont', 'risk', 'title', 'tax', 'professor', 'june', 'entreprise', 'residence', 'collective', 'usage', 'mail', 'postale', 'fax', 'disease', 'taste', 'april', 'lieu', 'social', 'charges', 'passeport', 'pris', 'could', 'usa', 'plafonnee', 'must', 'make', 'travel', 'urssaf', 'associate', 'days', 'cas', 'table', 'nationalite', 'personnel', 'sans', 'winston', 'payment', 'account', 'market', 'famille', 'contact', 'society', 'long', 'government', 'children', 'using', 'distribution', 'case', 'including', 'contribution', 'key', 'status', 'specific', 'exposure', 'note', 'eduldocs', 'september', 'activity', 'phone', 'fra', 'women', 'jour', 'titulaire', 'management', 'indemnite', 'position', 'points', 'copy', 'prenom', 'special', 'effects', 'members', 'point', 'assistant', 'prime', 'pre', 'end', 'science', 'plan', 'august', 'ing', 'advertising', 'marie', 'process', 'hospital', 'good', 'way', 'flavor', 'october', 'experience', 'publication', 'yes', 'reference', 'additional', 'attached', 'vice', 'income', 'marketing', 'deplacements', 'average', 'right', 'protein', 'growth', 'groups', 'volume', 'con', 'international', 'free', 'diet', 'pat', 'drug', 'country', 'review', 'previous', 'based', 'complete', 'times', 'july', 'different', 'staff', 'federal', 'questions', 'calcium', 'und', 'best', 'length', 'companies', 'today', 'etre', 'back', 'fiche', 'programs', 'levels', 'training', 'preleve', 'foods', 'designation', 'costs', 'pro', 'required', 'however', 'associated', 'possible', 'within', 'sociales', 'action', 'ville', 'kool', 'retenues', 'agency', 'ohio', 'dept', 'found', 'sent', 'salariales', 'person', 'production', 'xxx', 'december', 'solde', 'clinical', 'monthly', 'november', 'survey', 'biochemistry', 'site', 'price', 'edf', 'sheet', 'direct', 'cent', 'control', 'virginia', 'percent', 'prepared', 'factors', 'full', 'category', 'sal', 'issue', 'salem', 'prenoms', 'acquis', 'transport', 'duree', 'rtbpfw', 'sugar', 'loss', 'final', 'approved', 'progress', 'record', 'senior', 'since', 'without', 'interest', 'evaluation', 'lights', 'determine', 'chicago', 'get', 'agreement', 'venue', 'budget', 'several', 'increase', 'need', 'sex', 'fire', 'formation', 'result', 'request', 'provide', 'maladies', 'reglement', 'support', 'quality', 'hand', 'human', 'consumer', 'molecular', 'schedule', 'men', 'increased', 'take', 'cells', 'notice', 'law', 'cases', 'added', 'display', 'scientific', 'chemicals', 'various', 'anciennete', 'cadre', 'bank', 'dry', 'important', 'reported', 'standard', 'promotion', 'set', 'die', 'proposed', 'february', 'ape', 'council', 'chemistry', 'member', 'mensuel', 'dupont', 'reduction', 'values', 'park', 'mode', 'effect', 'names', 'informations', 'invoice', 'legal', 'thank', 'every', 'charge', 'sodium', 'analysis', 'domicile', 'sciences', 'past', 'personal', 'begin', 'taxe', 'major', 'annual', 'canada', 'jean', 'return', 'rev', 'much', 'vieillesse', 'moviles', 'california', 'david', 'ind', 'professionnelles', 'four', 'accident', 'rod', 'horaire', 'let', 'held', 'environmental', 'gene', 'quarter', 'principal', 'evidence', 'comments', 'dated', 'purpose', 'attestation', 'samples', 'manager', 'materials', 'active', 'experimental', 'etc', 'louis', 'tion', 'jan', 'heart', 'mif', 'press', 'india', 'dear', 'preparation', 'least', 'records', 'related', 'approval', 'second', 'world', 'media', 'laboratory', 'dec', 'assets', 'foundation', 'menthol', 'bureau', 'help', 'reynolds', 'even', 'gaelle', 'eur', 'electricite', 'january', 'north', 'verse', 'entre', 'ask', 'administration', 'hormone', 'funds', 'san', 'none', 'produced', 'read', 'dut', 'white', 'location', 'rat', 'single', 'limited', 'htps', 'packs', 'soft', 'union', 'releve', 'activities', 'cedex', 'response', 'item', 'million', 'prioritization', 'common', 'annee', 'internet', 'population', 'next', 'compared', 'grade', 'fellow', 'system', 'accidents', 'cause', 'side', 'line', 'necessary', 'another', 'applicable', 'matricule', 'robert', 'unitaire', 'tipping', 'cours', 'role', 'significant', 'naissance', 'students', 'illinois', 'allegement', 'temps', 'sample', 'paid', 'others', 'question', 'post', 'addition', 'discussion', 'month', 'america', 'notes', 'details', 'animals', 'conference', 'art', 'room', 'provided', 'trade', 'fnal', 'correct', 'john', 'indemnites', 'care', 'frais', 'naf', 'maternite', 'prelevement', 'suppression', 'bap', 'technology', 'regular', 'missouri', 'solidarite', 'degree', 'building', 'months', 'rates', 'individuals', 'acid', 'effective', 'tumor', 'problem', 'chairman', 'home', 'death', 'elements', 'county', 'ans', 'physical', 'give', 'old', 'deplacement', 'substances', 'capital', 'upon', 'life', 'potential', 'societe', 'virement', 'basic', 'amounts', 'statement', 'limitation', 'items', 'fda', 'still', 'iban', 'although', 'faire', 'male', 'shown', 'coupon', 'currently', 'true', 'annuel', 'marlboro', 'taille', 'weeks', 'contract', 'cotisation', 'charles', 'higher', 'actual', 'continue', 'salaries', 'female', 'show', 'quantite', 'delivery', 'nov', 'model', 'terminate', 'field', 'unit', 'term', 'salarial', 'filler', 'merci', 'difference', 'nnk', 'incapacite', 'william', 'investigator', 'panel', 'graduate', 'great', 'consommation', 'terms', 'mouse', 'retenue', 'traitement', 'merit', 'brute', 'range', 'inter', 'financial', 'trimegestone', 'carton', 'tout', 'might', 'appel', 'submitted', 'mme', 'mouth', 'road', 'taken', 'doit', 'answer', 'thomas', 'house', 'variety', 'segment', 'family', 'eau', 'depuis', 'whether', 'supplementaires', 'anti', 'printing', 'profit', 'know', 'basis', 'approach', 'article', 'hotel', 'boston', 'face', 'itc', 'message', 'placed', 'bancaire', 'working', 'areas', 'size', 'procedure', 'class', 'confidential', 'mortality', 'brown', 'diseases', 'det', 'persons', 'ags', 'equity', 'extremely', 'gain', 'relationship', 'fur', 'ppm', 'pressure', 'expected', 'authorized', 'offer', 'projects', 'extreme', 'store', 'taxes', 'maelys', 'dose', 'fund', 'advisory', 'shares', 'independent', 'sous', 'figure', 'revised', 'green', 'min', 'massachusetts', 'parts', 'hazard', 'smith', 'content', 'estradiol', 'banque', 'act', 'either', 'cout', 'los', 'fish', 'completed', 'young', 'postdoctoral', 'letter', 'saint', 'station', 'king', 'small', 'agent', 'send', 'evolution', 'original', 'course', 'documents', 'ten', 'little', 'references', 'problems', 'recent', 'reason', 'court', 'black', 'associates', 'private', 'global', 'future', 'requested', 'mars', 'containing', 'prevoyance', 'main', 'cfr', 'countries', 'south', 'bic', 'absence', 'regulation', 'pay', 'chf', 'stock', 'reports', 'buy', 'subjects', 'opinion', 'den', 'nil', 'meetings', 'ofl', 'paul', 'operations', 'come', 'individual', 'retard', 'expense', 'proposal', 'milk', 'daily', 'email', 'selected', 'safety', 'brain', 'making', 'planning', 'doctors', 'prior', 'representative', 'industrial', 'corporate', 'corporation', 'schools', 'involved', 'carbon', 'say', 'better', 'interet', 'assistance', 'cumuls', 'iii', 'toxicology', 'estimated', 'greater', 'james', 'zip', 'consumers', 'newport', 'money', 'brands', 'responsible', 'impact', 'indicate', 'news', 'displays', 'later', 'central', 'estimate', 'postal', 'mice', 'differences', 'ads', 'west', 'identification', 'aut', 'cheque', 'third', 'biological', 'lorillard', 'cfc', 'tissue', 'cette', 'determined', 'commerce', 'job', 'ultra', 'glue', 'community', 'five', 'acc', 'impots', 'want', 'issues', 'avantages', 'police', 'target', 'receive', 'early', 'summary', 'richmond', 'allow', 'executive', 'univ', 'droits', 'included', 'retail', 'run', 'fact', 'doc', 'fin', 'dates', 'pierre', 'wednesday', 'netapayer', 'man', 'familiales', 'echelon', 'focus', 'indice', 'organization', 'rubrique', 'dollars', 'instructions', 'florida', 'likely', 'prevention', 'liee', 'print', 'adult', 'payees', 'dna', 'plug', 'apec', 'going', 'nitrogen', 'large', 'exposed', 'says', 'primes', 'lower', 'plafond', 'dust', 'tablets', 'vitro', 'nih', 'land', 'congress', 'supp', 'trial', 'grams', 'cont', 'percentage', 'introduction', 'sup', 'warning', 'indicated', 'camel', 'lab', 'hors', 'toulouse', 'recently', 'friday', 'conducted', 'ment', 'method', 'cash', 'entrez', 'courrier', 'wall', 'express', 'payable', 'cover', 'written', 'enfants', 'texas', 'shall', 'expenditures', 'academy', 'technical', 'libelle', 'official', 'niveau', 'entree', 'adhesive', 'continued', 'patients', 'logo', 'normal', 'numbers', 'presence', 'urgence', 'yield', 'totale', 'salariale', 'deduire', 'leading', 'classification', 'specification', 'honors', 'prochaine', 'vers', 'cpn', 'factor', 'personnalise', 'slight', 'respect', 'stores', 'mark', 'systems', 'chain', 'enclosed', 'chronological', 'reduced', 'vol', 'personnes', 'activite', 'weekly', 'designed', 'parents', 'real', 'certain', 'suite', 'established', 'plant', 'activation', 'fig', 'grant', 'michigan', 'ref', 'done', 'look', 'departement', 'pays', 'papers', 'scientist', 'mar', 'comment', 'image', 'open', 'mean', 'objective', 'salt']\n"
     ]
    }
   ],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=100000)\n",
    "words = list(w2v_model.wv.index_to_key)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a85e5f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "v=[]\n",
    "df_n=pd.DataFrame(columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ed621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3eaa91a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for e,j in enumerate(df.text_image):\n",
    "    mots=stop_words_filtering(tokenizer.tokenize(str(j).lower()))\n",
    "    for i in mots:\n",
    "        if i not in words:\n",
    "             \n",
    "            continue  \n",
    "       \n",
    "        else:\n",
    "            v.append(i)\n",
    "    df_n.loc[e]=' '.join(v)\n",
    "            \n",
    "    v=[]\n",
    "    \n",
    "            \n",
    "#mot_vu=w2v_model.wv['facture']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2cb4ce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1566,)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_n.text[3]\n",
    "df.text_image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "36109c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadoun\\AppData\\Local\\Temp\\ipykernel_10504\\1465323158.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  total_donnes.append(text_total/len(txt.split()))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_donnes=[]\n",
    "text_total=np.zeros([200])\n",
    "for txt in df_n.text:\n",
    "    for mots in txt.split():\n",
    "        text_total=text_total+w2v_model.wv[mots]\n",
    "    total_donnes.append(text_total/len(txt.split()))\n",
    "    text_total=np.zeros([200])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1f25f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_donnes=pd.DataFrame(np.array(total_donnes))\n",
    "total_donnes['type']=df.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "03d31453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.756984</td>\n",
       "      <td>0.317420</td>\n",
       "      <td>-0.248904</td>\n",
       "      <td>0.226631</td>\n",
       "      <td>-0.234701</td>\n",
       "      <td>0.390436</td>\n",
       "      <td>-0.086576</td>\n",
       "      <td>1.326710</td>\n",
       "      <td>-0.363297</td>\n",
       "      <td>0.073002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311198</td>\n",
       "      <td>0.572089</td>\n",
       "      <td>-0.305657</td>\n",
       "      <td>1.214558</td>\n",
       "      <td>-0.891684</td>\n",
       "      <td>0.855939</td>\n",
       "      <td>-0.860518</td>\n",
       "      <td>-0.631233</td>\n",
       "      <td>-0.090971</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.895900</td>\n",
       "      <td>0.026922</td>\n",
       "      <td>-0.241642</td>\n",
       "      <td>0.276303</td>\n",
       "      <td>-0.322610</td>\n",
       "      <td>0.510605</td>\n",
       "      <td>-0.262356</td>\n",
       "      <td>1.489211</td>\n",
       "      <td>-0.371322</td>\n",
       "      <td>0.160381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182875</td>\n",
       "      <td>0.429457</td>\n",
       "      <td>-0.602394</td>\n",
       "      <td>1.549363</td>\n",
       "      <td>-0.563384</td>\n",
       "      <td>0.728003</td>\n",
       "      <td>-0.681514</td>\n",
       "      <td>-0.806399</td>\n",
       "      <td>-0.143780</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.157067</td>\n",
       "      <td>1.079796</td>\n",
       "      <td>-0.697124</td>\n",
       "      <td>-0.058355</td>\n",
       "      <td>-0.645184</td>\n",
       "      <td>-0.045530</td>\n",
       "      <td>1.048800</td>\n",
       "      <td>1.414565</td>\n",
       "      <td>0.277924</td>\n",
       "      <td>0.420650</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539992</td>\n",
       "      <td>-0.267890</td>\n",
       "      <td>-1.782511</td>\n",
       "      <td>-0.383777</td>\n",
       "      <td>0.113479</td>\n",
       "      <td>-0.458142</td>\n",
       "      <td>-0.062219</td>\n",
       "      <td>0.934816</td>\n",
       "      <td>-0.763592</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.895900</td>\n",
       "      <td>0.026922</td>\n",
       "      <td>-0.241642</td>\n",
       "      <td>0.276303</td>\n",
       "      <td>-0.322610</td>\n",
       "      <td>0.510605</td>\n",
       "      <td>-0.262356</td>\n",
       "      <td>1.489211</td>\n",
       "      <td>-0.371322</td>\n",
       "      <td>0.160381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182875</td>\n",
       "      <td>0.429457</td>\n",
       "      <td>-0.602394</td>\n",
       "      <td>1.549363</td>\n",
       "      <td>-0.563384</td>\n",
       "      <td>0.728003</td>\n",
       "      <td>-0.681514</td>\n",
       "      <td>-0.806399</td>\n",
       "      <td>-0.143780</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.119172</td>\n",
       "      <td>0.430006</td>\n",
       "      <td>-0.700115</td>\n",
       "      <td>0.816359</td>\n",
       "      <td>0.739402</td>\n",
       "      <td>1.115554</td>\n",
       "      <td>0.147204</td>\n",
       "      <td>1.739983</td>\n",
       "      <td>-0.074337</td>\n",
       "      <td>-0.277010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208327</td>\n",
       "      <td>-0.709420</td>\n",
       "      <td>0.094753</td>\n",
       "      <td>0.992594</td>\n",
       "      <td>-0.952163</td>\n",
       "      <td>0.984138</td>\n",
       "      <td>-1.157638</td>\n",
       "      <td>0.039770</td>\n",
       "      <td>-0.628345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.756984  0.317420 -0.248904  0.226631 -0.234701  0.390436 -0.086576   \n",
       "1 -1.895900  0.026922 -0.241642  0.276303 -0.322610  0.510605 -0.262356   \n",
       "2  0.157067  1.079796 -0.697124 -0.058355 -0.645184 -0.045530  1.048800   \n",
       "3 -1.895900  0.026922 -0.241642  0.276303 -0.322610  0.510605 -0.262356   \n",
       "4 -0.119172  0.430006 -0.700115  0.816359  0.739402  1.115554  0.147204   \n",
       "\n",
       "          7         8         9  ...       191       192       193       194  \\\n",
       "0  1.326710 -0.363297  0.073002  ... -0.311198  0.572089 -0.305657  1.214558   \n",
       "1  1.489211 -0.371322  0.160381  ... -0.182875  0.429457 -0.602394  1.549363   \n",
       "2  1.414565  0.277924  0.420650  ... -0.539992 -0.267890 -1.782511 -0.383777   \n",
       "3  1.489211 -0.371322  0.160381  ... -0.182875  0.429457 -0.602394  1.549363   \n",
       "4  1.739983 -0.074337 -0.277010  ... -0.208327 -0.709420  0.094753  0.992594   \n",
       "\n",
       "        195       196       197       198       199  type  \n",
       "0 -0.891684  0.855939 -0.860518 -0.631233 -0.090971   0.0  \n",
       "1 -0.563384  0.728003 -0.681514 -0.806399 -0.143780   0.0  \n",
       "2  0.113479 -0.458142 -0.062219  0.934816 -0.763592   0.0  \n",
       "3 -0.563384  0.728003 -0.681514 -0.806399 -0.143780   0.0  \n",
       "4 -0.952163  0.984138 -1.157638  0.039770 -0.628345   0.0  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_donnes.shape\n",
    "total_donnes.dropna(inplace=True)\n",
    "total_donnes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47842118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "98faaa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=total_donnes.drop('type',axis=1)\n",
    "\n",
    "target=total_donnes.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "daedb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train, y_test=train_test_split(data,target,test_size=0.2,random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bb8c9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn import svm,model_selection,preprocessing\n",
    "scaler=preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "38c90143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_26 (Dense)            (None, 256)               51456     \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 23)                5911      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 123,159\n",
      "Trainable params: 123,159\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Dropout,Activation\n",
    "model=Sequential()\n",
    "model.add(Dense(units = 256,input_shape=(200,)))\n",
    "model.add((Activation('relu')))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 23, activation = \"softmax\"))\n",
    "model.summary()\n",
    "#clf=linear_model.LogisticRegression(C=)\n",
    "#clf.fit(X_train,y_train)\n",
    "#ac=AdaBoostClassifier(base_estimator=dtc,n_estimators=400)\n",
    "#ac.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0cd629ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "33/33 [==============================] - 1s 10ms/step - loss: 0.1935 - accuracy: 0.9591 - val_loss: 4.7364 - val_accuracy: 0.4806\n",
      "Epoch 2/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9514 - val_loss: 4.6234 - val_accuracy: 0.5233\n",
      "Epoch 3/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1649 - accuracy: 0.9582 - val_loss: 4.6789 - val_accuracy: 0.5155\n",
      "Epoch 4/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9669 - val_loss: 4.8675 - val_accuracy: 0.5194\n",
      "Epoch 5/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1469 - accuracy: 0.9611 - val_loss: 4.8685 - val_accuracy: 0.5349\n",
      "Epoch 6/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1496 - accuracy: 0.9611 - val_loss: 4.7244 - val_accuracy: 0.5349\n",
      "Epoch 7/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1627 - accuracy: 0.9621 - val_loss: 4.7790 - val_accuracy: 0.5078\n",
      "Epoch 8/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1364 - accuracy: 0.9640 - val_loss: 4.8633 - val_accuracy: 0.5271\n",
      "Epoch 9/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1960 - accuracy: 0.9630 - val_loss: 5.0589 - val_accuracy: 0.5116\n",
      "Epoch 10/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2479 - accuracy: 0.9582 - val_loss: 5.0303 - val_accuracy: 0.5310\n",
      "Epoch 11/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1774 - accuracy: 0.9504 - val_loss: 5.1410 - val_accuracy: 0.5233\n",
      "Epoch 12/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1716 - accuracy: 0.9669 - val_loss: 5.2055 - val_accuracy: 0.5465\n",
      "Epoch 13/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1412 - accuracy: 0.9718 - val_loss: 5.3088 - val_accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1336 - accuracy: 0.9698 - val_loss: 5.2554 - val_accuracy: 0.5233\n",
      "Epoch 15/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1429 - accuracy: 0.9630 - val_loss: 5.2569 - val_accuracy: 0.5388\n",
      "Epoch 16/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1221 - accuracy: 0.9708 - val_loss: 5.1977 - val_accuracy: 0.5388\n",
      "Epoch 17/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.9728 - val_loss: 5.2119 - val_accuracy: 0.5233\n",
      "Epoch 18/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1298 - accuracy: 0.9689 - val_loss: 5.3466 - val_accuracy: 0.5194\n",
      "Epoch 19/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1128 - accuracy: 0.9630 - val_loss: 5.4035 - val_accuracy: 0.5233\n",
      "Epoch 20/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9640 - val_loss: 5.4589 - val_accuracy: 0.5271\n",
      "Epoch 21/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1082 - accuracy: 0.9786 - val_loss: 5.5320 - val_accuracy: 0.5426\n",
      "Epoch 22/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1148 - accuracy: 0.9660 - val_loss: 5.5144 - val_accuracy: 0.5155\n",
      "Epoch 23/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9640 - val_loss: 5.4609 - val_accuracy: 0.5310\n",
      "Epoch 24/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1293 - accuracy: 0.9747 - val_loss: 5.5221 - val_accuracy: 0.5465\n",
      "Epoch 25/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1039 - accuracy: 0.9679 - val_loss: 5.7819 - val_accuracy: 0.5310\n",
      "Epoch 26/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1189 - accuracy: 0.9640 - val_loss: 5.9083 - val_accuracy: 0.5310\n",
      "Epoch 27/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1035 - accuracy: 0.9679 - val_loss: 5.8800 - val_accuracy: 0.5233\n",
      "Epoch 28/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9689 - val_loss: 5.7640 - val_accuracy: 0.5039\n",
      "Epoch 29/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1060 - accuracy: 0.9689 - val_loss: 5.8137 - val_accuracy: 0.5349\n",
      "Epoch 30/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1189 - accuracy: 0.9660 - val_loss: 5.9012 - val_accuracy: 0.5194\n",
      "Epoch 31/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0950 - accuracy: 0.9786 - val_loss: 6.2512 - val_accuracy: 0.5310\n",
      "Epoch 32/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0873 - accuracy: 0.9776 - val_loss: 6.3628 - val_accuracy: 0.5116\n",
      "Epoch 33/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1033 - accuracy: 0.9708 - val_loss: 6.3181 - val_accuracy: 0.5155\n",
      "Epoch 34/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.9776 - val_loss: 6.2215 - val_accuracy: 0.5116\n",
      "Epoch 35/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9805 - val_loss: 6.2544 - val_accuracy: 0.5349\n",
      "Epoch 36/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.9757 - val_loss: 6.2239 - val_accuracy: 0.5194\n",
      "Epoch 37/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0874 - accuracy: 0.9708 - val_loss: 6.3320 - val_accuracy: 0.5155\n",
      "Epoch 38/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9815 - val_loss: 6.3112 - val_accuracy: 0.5310\n",
      "Epoch 39/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.9718 - val_loss: 6.0653 - val_accuracy: 0.5155\n",
      "Epoch 40/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1398 - accuracy: 0.9679 - val_loss: 6.2513 - val_accuracy: 0.5233\n",
      "Epoch 41/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1476 - accuracy: 0.9689 - val_loss: 6.1806 - val_accuracy: 0.5039\n",
      "Epoch 42/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0940 - accuracy: 0.9698 - val_loss: 6.2834 - val_accuracy: 0.5078\n",
      "Epoch 43/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1083 - accuracy: 0.9640 - val_loss: 6.1757 - val_accuracy: 0.5155\n",
      "Epoch 44/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0982 - accuracy: 0.9689 - val_loss: 6.2586 - val_accuracy: 0.4845\n",
      "Epoch 45/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1112 - accuracy: 0.9708 - val_loss: 6.5170 - val_accuracy: 0.5116\n",
      "Epoch 46/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1276 - accuracy: 0.9650 - val_loss: 6.3709 - val_accuracy: 0.5194\n",
      "Epoch 47/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1174 - accuracy: 0.9679 - val_loss: 6.4420 - val_accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1500 - accuracy: 0.9669 - val_loss: 6.4652 - val_accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9640 - val_loss: 6.4561 - val_accuracy: 0.5078\n",
      "Epoch 50/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1207 - accuracy: 0.9660 - val_loss: 6.6281 - val_accuracy: 0.4845\n",
      "Epoch 51/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1495 - accuracy: 0.9630 - val_loss: 6.4519 - val_accuracy: 0.5233\n",
      "Epoch 52/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.9611 - val_loss: 6.6005 - val_accuracy: 0.4961\n",
      "Epoch 53/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1585 - accuracy: 0.9708 - val_loss: 6.7360 - val_accuracy: 0.4922\n",
      "Epoch 54/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9718 - val_loss: 6.6298 - val_accuracy: 0.4767\n",
      "Epoch 55/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1000 - accuracy: 0.9679 - val_loss: 6.6145 - val_accuracy: 0.5116\n",
      "Epoch 56/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1466 - accuracy: 0.9737 - val_loss: 6.8547 - val_accuracy: 0.5039\n",
      "Epoch 57/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9669 - val_loss: 6.7766 - val_accuracy: 0.5078\n",
      "Epoch 58/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9689 - val_loss: 6.6993 - val_accuracy: 0.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1180 - accuracy: 0.9737 - val_loss: 6.8090 - val_accuracy: 0.5155\n",
      "Epoch 60/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1209 - accuracy: 0.9747 - val_loss: 6.8636 - val_accuracy: 0.5116\n",
      "Epoch 61/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1134 - accuracy: 0.9660 - val_loss: 6.8130 - val_accuracy: 0.5194\n",
      "Epoch 62/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1089 - accuracy: 0.9640 - val_loss: 7.0156 - val_accuracy: 0.4961\n",
      "Epoch 63/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1190 - accuracy: 0.9669 - val_loss: 7.1962 - val_accuracy: 0.5116\n",
      "Epoch 64/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0949 - accuracy: 0.9747 - val_loss: 7.1488 - val_accuracy: 0.5155\n",
      "Epoch 65/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1050 - accuracy: 0.9747 - val_loss: 7.1304 - val_accuracy: 0.5194\n",
      "Epoch 66/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0899 - accuracy: 0.9718 - val_loss: 7.1836 - val_accuracy: 0.5194\n",
      "Epoch 67/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0958 - accuracy: 0.9728 - val_loss: 7.1942 - val_accuracy: 0.5194\n",
      "Epoch 68/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9805 - val_loss: 7.1689 - val_accuracy: 0.5078\n",
      "Epoch 69/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9815 - val_loss: 7.2590 - val_accuracy: 0.5194\n",
      "Epoch 70/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9776 - val_loss: 7.3330 - val_accuracy: 0.5155\n",
      "Epoch 71/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.9815 - val_loss: 7.4683 - val_accuracy: 0.5078\n",
      "Epoch 72/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0668 - accuracy: 0.9776 - val_loss: 7.4675 - val_accuracy: 0.5155\n",
      "Epoch 73/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 0.9737 - val_loss: 7.3288 - val_accuracy: 0.5194\n",
      "Epoch 74/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1081 - accuracy: 0.9698 - val_loss: 7.2696 - val_accuracy: 0.5039\n",
      "Epoch 75/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0866 - accuracy: 0.9757 - val_loss: 7.3528 - val_accuracy: 0.4922\n",
      "Epoch 76/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9805 - val_loss: 7.3809 - val_accuracy: 0.5194\n",
      "Epoch 77/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.9786 - val_loss: 7.3395 - val_accuracy: 0.5155\n",
      "Epoch 78/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9776 - val_loss: 7.2877 - val_accuracy: 0.5155\n",
      "Epoch 79/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9815 - val_loss: 7.2259 - val_accuracy: 0.5194\n",
      "Epoch 80/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0896 - accuracy: 0.9718 - val_loss: 7.1930 - val_accuracy: 0.5233\n",
      "Epoch 81/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9747 - val_loss: 7.3453 - val_accuracy: 0.5233\n",
      "Epoch 82/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9776 - val_loss: 7.3386 - val_accuracy: 0.5194\n",
      "Epoch 83/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9767 - val_loss: 7.3739 - val_accuracy: 0.5233\n",
      "Epoch 84/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0913 - accuracy: 0.9786 - val_loss: 7.2618 - val_accuracy: 0.5116\n",
      "Epoch 85/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9815 - val_loss: 7.1326 - val_accuracy: 0.5155\n",
      "Epoch 86/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0702 - accuracy: 0.9805 - val_loss: 7.0886 - val_accuracy: 0.5155\n",
      "Epoch 87/100\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.9864 - val_loss: 7.2385 - val_accuracy: 0.5271\n",
      "Epoch 88/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.9786 - val_loss: 7.2908 - val_accuracy: 0.5233\n",
      "Epoch 89/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0539 - accuracy: 0.9796 - val_loss: 7.3296 - val_accuracy: 0.5155\n",
      "Epoch 90/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0559 - accuracy: 0.9815 - val_loss: 7.4058 - val_accuracy: 0.5310\n",
      "Epoch 91/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0561 - accuracy: 0.9786 - val_loss: 7.6246 - val_accuracy: 0.5233\n",
      "Epoch 92/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0721 - accuracy: 0.9689 - val_loss: 7.8606 - val_accuracy: 0.4922\n",
      "Epoch 93/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0656 - accuracy: 0.9747 - val_loss: 7.7492 - val_accuracy: 0.5233\n",
      "Epoch 94/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.9786 - val_loss: 7.7631 - val_accuracy: 0.5116\n",
      "Epoch 95/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0783 - accuracy: 0.9767 - val_loss: 7.9684 - val_accuracy: 0.5039\n",
      "Epoch 96/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0627 - accuracy: 0.9757 - val_loss: 8.0834 - val_accuracy: 0.5039\n",
      "Epoch 97/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.9718 - val_loss: 8.1335 - val_accuracy: 0.5155\n",
      "Epoch 98/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0783 - accuracy: 0.9737 - val_loss: 8.1834 - val_accuracy: 0.5116\n",
      "Epoch 99/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9747 - val_loss: 8.2880 - val_accuracy: 0.5116\n",
      "Epoch 100/100\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.0915 - accuracy: 0.9689 - val_loss: 8.1956 - val_accuracy: 0.4845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f04dc44ca0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.fit(X_train,y_train,epochs = 50, batch_size = 32,validation_data = [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "3cb7563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 2ms/step - loss: 8.1956 - accuracy: 0.4845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.195639610290527, 0.4844961166381836]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
